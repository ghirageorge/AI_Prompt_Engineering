{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aab4cbb7-d654-4a90-96ff-612455e5a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer: o200k_base\n",
      "Original sentance: Hello world, how are you?\n",
      "--------------------------------------------------\n",
      "Tokens ID: [13225, 2375, 11, 1495, 553, 481, 30]\n",
      "Decoded Tokens: ['Hello', ' world', ',', ' how', ' are', ' you', '?']\n",
      "total tokens: 7\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "print (f\"Successfully loaded tokenizer: {tokenizer.name}\")\n",
    "\n",
    "def analyze_sentence (sencente):\n",
    "    token_ids = tokenizer.encode(sentence)\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "\n",
    "print (f\"Original sentance: {sentence}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Tokens ID: {token_ids}\")\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "print(f\"total tokens: {len(token_ids)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4985782-d584-4e74-bc87-2ae6eb9e0c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentance: Hello world, how are you?\n",
      "--------------------------------------------------\n",
      "Tokens ID: [13225, 2375, 11, 1495, 553, 481, 30]\n",
      "Decoded Tokens: ['Hello', ' world', ',', ' how', ' are', ' you', '?']\n",
      "total tokens: 7\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello world, how are you?\"\n",
    "\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "\n",
    "print (f\"Original sentance: {sentence}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Tokens ID: {token_ids}\")\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "print(f\"total tokens: {len(token_ids)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "827455c5-5e36-4c58-b394-efa512ad5fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e305115-bd1f-4188-a497-e9d993c2ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentance: Hello world, how are you?\n",
      "--------------------------------------------------\n",
      "Tokens ID: [40, 939, 625, 290, 3641, 0, 357, 448, 625, 1952, 290, 4246, 0]\n",
      "Decoded Tokens: ['I', ' am', ' not', ' the', ' body', '!', ' I', ' an', ' not', ' even', ' the', ' mind', '!']\n",
      "total tokens: 13\n"
     ]
    }
   ],
   "source": [
    "complex_string=\"I am not the body! I an not even the mind!\"\n",
    "\n",
    "token_ids_complex = tokenizer.encode(complex_string)\n",
    "\n",
    "token_ids_complex = tokenizer.encode(complex_string)\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in token_ids_complex]\n",
    "\n",
    "print (f\"Original sentance: {sentence}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Tokens ID: {token_ids_complex}\")\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "print(f\"total tokens: {len(token_ids_complex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1a27b5f-2923-43cd-a4f1-59225d1a6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentance: def process_data(data: str) -> str:\n",
      "--------------------------------------------------\n",
      "Tokens ID: [13225, 2375, 11, 1495, 553, 481, 30]\n",
      "Decoded Tokens: ['I', ' am', ' not', ' the', ' body', '!', ' I', ' an', ' not', ' even', ' the', ' mind', '!']\n",
      "total tokens: 7\n"
     ]
    }
   ],
   "source": [
    "code_snippet = \"def process_data(data: str) -> str:\"\n",
    "\n",
    "analyze_sentence(code_snippet)\n",
    "\n",
    "print (f\"Original sentance: {code_snippet}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Tokens ID: {token_ids}\")\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "print(f\"total tokens: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c824122-9134-4baa-b277-5862b9a70c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
