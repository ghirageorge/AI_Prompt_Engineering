{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516a1796-da83-4705-9ce2-ced58098c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt response:\n",
      "**|A| Day| in| the| Life| of| Sam| the| Programmer|**\n",
      "\n",
      "|The| sun| peek|ed| through| the| curtains|,| slicing| through| the| morning| fog|.| Sam|'s| alarm| buzz|ed| with| the| same| persistent| rhythm| that| had| become| as| familiar| as| the| lines| of| code| she| wrote|.| She| reached| for| her| phone| to| silence| it|,| gro|aning| slightly| as| she| sat| up|.| Another| day| of| debugging| and| deployment| awaited| her|.\n",
      "\n",
      "|After| a| quick| breakfast| of| toast| and| coffee|,| Sam| settled| into| her| small| but| cozy| home| office|,| clutter|ed| with| m|ement|os| from| old| projects| and| a| white|board| filled| with| half|-er|ased| diagrams|.| She| opened| her| laptop|,| its| screen| flick|ering| to| life| with| the| familiar| glow| of| her| coding| environment|.| \n",
      "\n",
      "|First| on| her| agenda| was| solving| the| issue| that| had| arisen| the| night| before|.| A| bug| in| the| e|-commerce| application| she| was| working| on| had| sent| a| few| users| into| a| digital| rabbit| hole|,| and| her| inbox| was| flooded| with| confused| customer| emails|.| Sam| took| a| deep| breath| and| dove| into| the| code|,| s|ifting| through| lines| of| logic| with| a| cup| of| coffee| by| her| side|.| \n",
      "\n",
      "|As| the| hours| rolled| on|,| she| fell| into| the| rhythm| of| ke|yst|rokes| and| late|-m|orning| radio| tunes|.| With| every| passing| line| of| code|,| the| fog| of| frustration| lifted|.| Eventually|,| she| spotted| the| error|—a| simple| variable| mis|assignment|.| After| a| swift| fix| and| a| double|-check| of| her| test| cases|,| she| pushed| the| update|.| A| small| cheer| escaped| her| lips| as| the| bug| was| resolved|,| and| the| world| was| once| again| right| in| the| digital| marketplace|.\n",
      "\n",
      "|With| that| victory| behind| her|,| Sam| took| a| short| break| to| stretch| her| legs|.| She| glanced| out| the| window|,| watching| the| trees| sway| in| the| breeze|,| a| stark| contrast| to| her| indoor| world| of| syntax| and| logic|.| Inspired| by| the| view|,| she| decided| to| take| her| lunch| outside|,| enjoying| the| fresh| air| and| the| sounds| of| the| neighborhood|.\n",
      "\n",
      "|Returning| to| her| desk|,| she| switched| gears| to| a| side| project|—a| personal| website| redesign| she| had| been| excited| about|.| This| was| where| Sam| let| her| creativity| flourish|,| l|acing| vibrant| colors| with| smooth| animations|.| As| she| crafted| her| vision| into| reality|,| hours| slipped| by| und|etected|.\n",
      "\n",
      "|As| evening| approached|,| Sam| joined| a| virtual| team| meeting|,| where| she| shared| progress| updates| and| collaborated| on| strategy|.| The| camar|aderie| of| her| teammates| always| uplift|ed| her| spirits|,| and| before| long|,| they| were| brainstorming| solutions| for| an| upcoming| feature| release|.\n",
      "\n",
      "|After| the| meeting|,| Sam| felt| satisfied| but| aware| that| it| would| be| a| late| night|.| She| returned| to| her| code|,| fueled| by| leftover| Thai| food| and| a| tro|ve| of| energy| drinks|.| With|"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKEN = 500\n",
    "\n",
    "prompt_story = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story about a typical day in a programer's life\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=prompt_story,\n",
    "    max_tokens=MAX_TOKEN,\n",
    "    stream=True\n",
    "          \n",
    ")\n",
    "\n",
    "print (\"prompt response:\")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858607ee-e22e-412a-ad46-e4e823feb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = [\n",
    "    {\n",
    "          \"role\": \"user\",\n",
    "        \"content\": \"The capital of Frace is\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_capital = litellm.completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=PROMPT,\n",
    "    max_tokens=1,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a7be51b-1526-4177-98ef-f1d871fe374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The capital of Frace is\n",
      "The model chose the token: The\n",
      "Here are the top 5 most likely tokens:\n",
      "   - Token: 'The, Prob:  99.966424%'\n",
      "   - Token: 'It, Prob:  0.033535%'\n",
      "   - Token: 'I, Prob:  0.000014%'\n",
      "   - Token: ' The, Prob:  0.000009%'\n",
      "   - Token: 'There, Prob:  0.000009%'\n"
     ]
    }
   ],
   "source": [
    "top_logprobs = response_capital.choices[0].logprobs.content[0].top_logprobs\n",
    "chosen_token = response_capital.choices[0].message.content\n",
    "\n",
    "print(f\"Prompt: {PROMPT[0][\"content\"]}\")\n",
    "print(f\"The model chose the token: {chosen_token}\")\n",
    "print(\"Here are the top 5 most likely tokens:\")\n",
    "\n",
    "for logprob in top_logprobs:\n",
    "      prob_percentage = math.exp(logprob.logprob) * 100\n",
    "      print(f\"   - Token: '{logprob.token}, Prob: {prob_percentage: .6f}%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f1f47-e772-49ae-9cf9-e7dc36d1a295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
