{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89c28c-b7f8-4f53-b6c7-f83bd88fd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use a powerful, model to act as your expert prompt engineering partner. \n",
    "Its task is not to solve the final problem, but to generate a high-quality, \n",
    "optimized prompt that you can then use for your actual task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5a4546-4272-486f-b8dd-a1445863eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete for gpt-5. Helper functions and code context are ready.\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-5\"\n",
    "MAX_TOKENS_DEFAULT = 5001\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Setup complete for gpt-5. Helper functions and code context are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af9851da-02eb-45fd-8eeb-bdbee3a628b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "master_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are an expert prompt engineer, and your task is to create a high-quality,\n",
    "        optimized system prompt based on a user's specification.\n",
    "\n",
    "        ## Process:\n",
    "\n",
    "        1. Start by asking me to provide the task I want you to generate a prompt for.\n",
    "        2. Once I provide the task, ask me questions to clarify any doubts or missing information.\n",
    "        3. Once you have the necessary information, create a detailed and effective system prompt that\n",
    "            I can use with an AI system to tackle the task at hand.\n",
    "        4. When I write \"GENERATE\", you will generate the final prompt based on the information\n",
    "            we have discussed.\n",
    "\n",
    "        ## Generated prompt output rules:\n",
    "        * Output only the text of the generated prompt between <prompt>[GENERATED PROMPT]</prompt>\n",
    "        * Do not output any triple backticks nor code blocks, just the contents of the generated prompt \n",
    "            between the XML tags.\n",
    "        * Leverage markdown and prompt engineering best practices to structure the prompt.\n",
    "\n",
    "        ## Examples of responses for generated prompts:\n",
    "        <prompt>\n",
    "        You are a knowledgeable and supportive AI DevOps tutor specializing in automation, observability, and AWS. Your role is to guide a beginner who is currently learning the basics of AWS and aims to achieve the AWS Certified Developer - Associate certification. \n",
    "\n",
    "        Provide practical, hands-on learning experiences and real-world project ideas that align with my learning goals. Break down complex topics into understandable segments and include the following: \n",
    "        \n",
    "        1. **Foundational Concepts**: Explain essential DevOps principles and practices with a focus on automation and observability.\n",
    "        2. **AWS Services**: Introduce relevant AWS services, their use cases, and how they integrate into DevOps workflows.\n",
    "        3. **Real-World Projects**: Suggest projects that help apply theoretical knowledge in practical contexts, actively involving automation and observability.\n",
    "        4. **Study Resources**: Share study materials, tools, and best practices to prepare for the AWS Certified Developer - Associate exam.\n",
    "        5. **Interactive Exercises**: Create exercises that reinforce concepts and give me hands-on experience with AWS and DevOps practices.\n",
    "        \n",
    "        Encourage questions and provide answers constructively, promoting a clear understanding and a smooth learning journey towards certification.\n",
    "        </prompt>\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1e97c4-bddf-404b-90f5-30ea501e9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interactive prompt generation session...\n",
      "Type 'quit' to exit the conversation.\n",
      "\n",
      "AI:  What task would you like me to generate a prompt for? Once you share it, I’ll ask a few clarifying questions to tailor the prompt precisely to your needs.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I'm thinking about some template for prompting, that can be used and adapted to user needs. Like a tool box full with prompt templates: For ex: I've identified a bug and i open the bug-template that contains all that i need in order to specify and then obtain from AI. How to do that?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Great idea—a reusable prompt toolbox is very powerful. Per our process, let’s start by picking the first task you want a prompt for.\n",
      "\n",
      "Which template would you like to create first?\n",
      "- Example options: Bug report intake, Bug triage/reproduction, Feature request, Code review, Debugging assistant, Research brief, Meeting summary, PR description, Data analysis, Brainstorming.\n",
      "\n",
      "If you want to start with a “Bug template,” please confirm and answer the questions below. If you prefer a different template, tell me which and I’ll tailor the questions.\n",
      "\n",
      "General toolbox questions\n",
      "1) Target platform(s): Which AI system(s) will use these prompts (e.g., ChatGPT, Claude, local LLM)? Any context window limits?\n",
      "2) Template format: Do you want\n",
      "   - System prompts for the AI’s behavior,\n",
      "   - User-fillable templates with placeholders,\n",
      "   - Or both? Any preference for Markdown vs JSON?\n",
      "3) Reusability: Any variable/placeholder naming convention you prefer (e.g., {{APP_NAME}}, <<ENV>>) and a standard section order?\n",
      "4) Integrations/outputs: Should outputs align with tools (e.g., Jira/GitHub fields, JSON schema)?\n",
      "5) Language/tone: Single language or multilingual? Formal vs conversational?\n",
      "6) Constraints: Privacy/redaction needs? No external browsing/tools? Time limits?\n",
      "7) Priorities: Which 3–5 templates do you want in v1 of the toolbox?\n",
      "\n",
      "If starting with a Bug template, please clarify\n",
      "1) Purpose/scope:\n",
      "   - a) Bug report intake (collect complete info),\n",
      "   - b) Triage (severity/priority/area/component),\n",
      "   - c) Reproduction guide,\n",
      "   - d) Root-cause hypothesis and fix proposals,\n",
      "   - e) Ticket drafting for Jira/GitHub.\n",
      "2) Users: Who will interact with it (QA engineers, developers, support agents, end users)?\n",
      "3) Required fields: Which fields must the template capture? Common options:\n",
      "   - Title, Summary, Expected vs Actual behavior, Steps to Reproduce, Environment (app version, OS/browser, device, locale), Logs/stack traces, Screenshots/attachments, Severity/Impact, Frequency, Regression info, Workarounds, Related tickets/PRs, Component/Area, Test data, Network conditions.\n",
      "4) Output format:\n",
      "   - Markdown with clear headings,\n",
      "   - JSON (provide schema if you have one),\n",
      "   - Jira/GitHub issue fields (provide mapping or example).\n",
      "5) Follow-up behavior: Should the AI ask for missing/ambiguous details before finalizing?\n",
      "6) Style/voice: Any tone or phrasing guidelines? (e.g., concise, actionable, no blame)\n",
      "7) Examples: Do you have a sample bug report to emulate? Paste one if available.\n",
      "8) Constraints: Any PII to redact? Any banned content? Any company-specific taxonomy (severity levels, components) to embed?\n",
      "9) Acceptance criteria: What makes a “complete” bug report for you? Provide a checklist if possible.\n",
      "10) Tools: Can the AI run code, browse, or call APIs? Or is it text-only?\n",
      "\n",
      "Once you provide answers (or say which template to start with), I’ll draft the optimized system prompt. When you’re ready for the final, say “GENERATE” and I’ll output it between the required <prompt>[GENERATED PROMPT]</prompt> tags.\n",
      "--------------------------------------------------\n",
      "\n",
      "Final prompt has been generated and stored!\n",
      "============================================================\n",
      "\n",
      "Generated prompt:\n",
      "\n",
      "[GENERATED PROMPT]\n"
     ]
    }
   ],
   "source": [
    "conversation_history = master_prompt.copy()\n",
    "generated_prompt = None\n",
    "\n",
    "print(\"Starting interactive prompt generation session...\")\n",
    "print(\"Type 'quit' to exit the conversation.\\n\")\n",
    "\n",
    "initial_response = get_completion(conversation_history)\n",
    "print(\"AI: \", initial_response)\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"stop\"]:\n",
    "        print(\"Ending conversation.\")\n",
    "        break\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input\n",
    "    })\n",
    "\n",
    "    ai_response = get_completion(conversation_history)\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ai_response\n",
    "    })\n",
    "\n",
    "    print(\"AI: \", ai_response)\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    if \"<prompt>\" in ai_response:\n",
    "        generated_prompt = ai_response.split(\"<prompt>\")[1].rsplit(\"</prompt>\")[0].strip()\n",
    "        print(\"Final prompt has been generated and stored!\")\n",
    "        break\n",
    "\n",
    "if generated_prompt:\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    print(\"Generated prompt:\\n\")\n",
    "    print(generated_prompt)\n",
    "else:\n",
    "    print(\"No final prompt was generated in this session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860537da-2c5a-43d2-b8f6-4f1b1c902b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
