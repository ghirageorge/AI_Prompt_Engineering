{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95593163-a37e-49a0-bf63-d48aa004c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We break down a complex task into a sequence of smaller, focused sub-tasks. Each sub-task is handled by its own dedicated prompt, creating a \"pipeline\" where the output of one step becomes the input for the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369de3a9-8b0e-48ba-b650-7e53845c722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Helper functions and code context are ready.\n"
     ]
    }
   ],
   "source": [
    "# Identify bugs in a piece of code.\n",
    "\n",
    "import litellm\n",
    "import inspect\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS_DEFAULT = 500\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS_DEFAULT,\n",
    "    **kwargs\n",
    "):\n",
    "    if \"gpt-5\" in model:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "    else:\n",
    "        kwargs[\"max_tokens\"] = max_tokens\n",
    "        \n",
    "    parsed_messages = []\n",
    "\n",
    "    if type(prompt) is str:\n",
    "        parsed_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        parsed_messages = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=parsed_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_average_score(student_records):\n",
    "    total_score = 0\n",
    "    for record in student_records:\n",
    "        total_score += record['score']\n",
    "\n",
    "    return total_score / len(student_records)\n",
    "\n",
    "CODE_TO_PROCESS = inspect.getsource(get_average_score)\n",
    "\n",
    "print(\"Setup complete. Helper functions and code context are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fec9d3-eca1-42eb-9665-bd5865844602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Identifying bugs ---\n",
      "Identified issues:\n",
      "{\n",
      "    \"bugs\": [\n",
      "        \"The code does not handle the case where student_records is an empty list, which would lead to a ZeroDivisionError when dividing by len(student_records).\",\n",
      "        \"There is no check to ensure that each record in student_records contains the key 'score', which could lead to a KeyError if a record is missing this key.\",\n",
      "        \"The indentation is not consistent, making the function definition and the for loop improperly formatted, which will result in an IndentationError when executing the code.\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# step 1: prompt to identity bugs in code (in QA can be to identify bugs in req, in TD, etc..)\n",
    "prompt_bug_identification = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are a master code auditor.\n",
    "        Your sole task is to analyze Python code and identify potential bugs, errors, and edge-cases.\n",
    "\n",
    "        Do NOT suggest fixes.\n",
    "\n",
    "        Output your findings as a JSON object with the following schema:\n",
    "        \n",
    "        {\n",
    "            \"bugs\": list(str)\n",
    "        }\n",
    "        \"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(f\"\"\"\n",
    "        Please analyze the following Python code:\n",
    "\n",
    "        ```python\n",
    "        {CODE_TO_PROCESS}\n",
    "        ```\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Step 1: Identifying bugs ---\")\n",
    "identified_bugs_json = get_completion(\n",
    "    prompt_bug_identification,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "if identified_bugs_json:\n",
    "    print(\"Identified issues:\")\n",
    "    print(identified_bugs_json)\n",
    "    identified_bugs = json.loads(identified_bugs_json).get(\"bugs\", [])\n",
    "else:\n",
    "    identified_bugs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35e76b0-eb6f-48ba-a7ef-0d6b961bc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Refactoring the code ---\n",
      "Refactored code:\n",
      "def get_average_score(student_records: list[dict[str, float]]) -> float:\n",
      "    \"\"\"\n",
      "    Calculate the average score from a list of student records.\n",
      "\n",
      "    Each student record must be a dictionary containing a 'score' key. \n",
      "    An empty list or missing 'score' keys will raise appropriate exceptions.\n",
      "\n",
      "    :param student_records: A list of dictionaries containing student scores\n",
      "    :return: The average score of the students\n",
      "    :raises ValueError: If student_records is empty or if any record is missing the 'score' key\n",
      "    \"\"\"\n",
      "    if not student_records:\n",
      "        raise ValueError(\"The student_records list is empty.\")\n",
      "\n",
      "    total_score = 0\n",
      "    for record in student_records:\n",
      "        if 'score' not in record:\n",
      "            raise KeyError(\"Each record must contain a 'score' key.\")\n",
      "        total_score += record['score']\n",
      "\n",
      "    return total_score / len(student_records)\n"
     ]
    }
   ],
   "source": [
    "# step 2. Having bugs identified, we want to fix the code. \n",
    "prompt_refactoring = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are a senior Python developer.\n",
    "        Your task is to refactor code to fix identified issues and improve its quality and robustness.\n",
    "        \"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(f\"\"\"\n",
    "        Please refactor the following Python code to fix the idenfied issues.\n",
    "        Add a docstring and type hints.\n",
    "\n",
    "        ## Original code\n",
    "        ```python\n",
    "        {CODE_TO_PROCESS}\n",
    "        ```\n",
    "\n",
    "        ## Identified issues\n",
    "        {\"* \".join(identified_bugs)}\n",
    "\n",
    "        ## Response instructions\n",
    "        * Your response ahould be only the complete, refactored Python code.\n",
    "        * Do not output any triple backticks nor code blocks, just directly the code.\n",
    "        * Output only valid Python code.\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Step 2: Refactoring the code ---\")\n",
    "refactored_code = get_completion(prompt_refactoring)\n",
    "\n",
    "if refactored_code:\n",
    "    print(\"Refactored code:\")\n",
    "    print(refactored_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d100f724-7ddc-4f8c-b195-60343d437ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Generating unit tests ---\n",
      "Generated tests:\n",
      "```xml\n",
      "<preamble>\n",
      "    Unit tests for the get_average_score function that computes the average score from student records.\n",
      "</preamble>\n",
      "<reasoning>\n",
      "    The tests are designed to verify the main functionality of the function, ensuring that it correctly calculates the average score, handles empty lists, and raises exceptions for missing 'score' keys. These tests will ensure robustness against typical user errors and edge cases.\n",
      "</reasoning>\n",
      "<edge_cases>\n",
      "    1. An empty list of student records, which should raise a ValueError.\n",
      "    2. A list containing one or more records that do not include the 'score' key, which should raise a KeyError.\n",
      "    3. A list with only one student record, ensuring it calculates the average correctly as the score of that single student.\n",
      "    4. A list with valid scores, including both positive and negative values, to confirm correct averaging.\n",
      "</edge_cases>\n",
      "<python_code>\n",
      "    import pytest\n",
      "\n",
      "    from your_module import get_average_score  # Replace with the actual import statement\n",
      "\n",
      "    def test_average_with_valid_scores():\n",
      "        records = [{'score': 80}, {'score': 90}, {'score': 70}]\n",
      "        assert get_average_score(records) == (80 + 90 + 70) / 3\n",
      "\n",
      "    def test_average_with_one_record():\n",
      "        records = [{'score': 100}]\n",
      "        assert get_average_score(records) == 100.0\n",
      "\n",
      "    def test_average_with_negative_scores():\n",
      "        records = [{'score': -10}, {'score': 20}, {'score': 30}]\n",
      "        assert get_average_score(records) == ( -10 + 20 + 30) / 3\n",
      "\n",
      "    def test_empty_student_records():\n",
      "        with pytest.raises(ValueError, match=\"The student_records list is empty.\"):\n",
      "            get_average_score([])\n",
      "\n",
      "    def test_missing_score_key():\n",
      "        records = [{'score': 90}, {'name': 'Alice'}]  # Second record missing 'score'\n",
      "        with pytest.raises(KeyError, match=\"Each record must contain a 'score' key.\"):\n",
      "            get_average_score(records)\n",
      "\n",
      "    def test_all_records_missing_score_key():\n",
      "        records = [{'name': 'Alice'}, {'name': 'Bob'}]  # No records have 'score'\n",
      "        with pytest.raises(KeyError, match=\"Each record must contain a 'score' key.\"):\n",
      "            get_average_score(records)\n",
      "</python_code>\n",
      "<conclusion>\n",
      "    The outlined tests comprehensively evaluate the get_average_score function, ensuring correctness under normal conditions and robustness against various edge cases. These tests will help maintain the integrity of the function through modifications and upgrades.\n",
      "</conclusion>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# step 3: create tests for fixed code. \n",
    "prompt_generate_unit_tests = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\n",
    "        You are an expert QA engineer specializing in Python.\n",
    "        Your task is to write a comprehensive suite of unit tests for a given function.\n",
    "        You always use the pytest framework.\n",
    "        \"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dedent(f\"\"\"\n",
    "        Please write unit tests for the following refactored Python function.\n",
    "\n",
    "        The tests should cover the main functionality and edge-cases.\n",
    "        \n",
    "        Output the content in XML strictly following the structure in the Output section.\n",
    "        Do not output any additional content outside of the XML tags.\n",
    "\n",
    "        ## Refactored code\n",
    "        {refactored_code}\n",
    "\n",
    "        ## Output\n",
    "        <preamble>\n",
    "            Add your preamble here.\n",
    "        </preamble>\n",
    "        <reasoning>\n",
    "            Add your reasoning here.\n",
    "        </reasoning>\n",
    "        <edge_cases>\n",
    "            Add all identified edge-cases here.\n",
    "        </edge_cases>\n",
    "        <python_code>\n",
    "            Add the test code here.\n",
    "        </python_code>\n",
    "        <conclusion>\n",
    "            Add the conclusion here\n",
    "        </conclusion>\n",
    "        \"\"\")\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- Step 3: Generating unit tests ---\")\n",
    "unit_tests = get_completion(prompt_generate_unit_tests, max_tokens=2000)\n",
    "\n",
    "if unit_tests:\n",
    "    print(\"Generated tests:\")\n",
    "    print(unit_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018efa0f-1eeb-4207-a86f-2c1bd437e9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
